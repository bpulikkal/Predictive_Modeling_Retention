{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the python code for predicting the retention of an auto insurance policy. Specifically, we are predicting whether a policy will renew to the 2nd term or not. Here it is framed as a supervised classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet = pd.read_csv('policydata_05222018.csv',engine='python')\n",
    "DataSet.head()\n",
    "DataSet.info()\n",
    "DataSet.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot numercial attributes in the dataset to understand the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Latitude and Longitude columns from zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uszipcode import ZipcodeSearchEngine\n",
    "DataSet['Latitude'] = DataSet['PostalCode'].apply(lambda pcode: ZipcodeSearchEngine().by_zipcode(pcode)['Latitude'])\n",
    "DataSet['Longitude'] = DataSet['PostalCode'].apply(lambda pcode: ZipcodeSearchEngine().by_zipcode(pcode)['Longitude'])\n",
    "\n",
    "#Replace missing values for Latitude & Longitude with its median\n",
    "DataSet.loc[DataSet['Latitude'].isna(),'Latitude']=DataSet['Latitude'].median()\n",
    "DataSet.loc[DataSet['Longitude'].isna(),'Longitude']=DataSet['Longitude'].median()\n",
    "\n",
    "DataSet = DataSet.drop(['PostalCode'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a category column (Here we are using written premium) in order to use for StratifiedShuffleSplit for splitting training & testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a category column for WP in order to use for StratifiedShuffleSplit for splitting training & testing datasets\n",
    "DataSet[\"WP_cat\"] = np.ceil(DataSet[\"WP\"] / 750) \n",
    "DataSet[\"WP_cat\"]. where(DataSet[\"WP_cat\"] < 5, 5.0, inplace = True)\n",
    "\n",
    "# Check the Distribution in the entire dataset\n",
    "DataSet['WP_cat'].value_counts() / len(DataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42) \n",
    "for train_index, test_index in split.split(DataSet, DataSet[\"WP_cat\"]): \n",
    "    strat_train_set = DataSet.loc[train_index] \n",
    "    strat_test_set = DataSet.loc[test_index]\n",
    "    \n",
    "\n",
    "#Check the Distribution in the test dataset\n",
    "strat_test_set['WP_cat'].value_counts() / len(strat_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop WP_cat column\n",
    "for set_ in (strat_train_set, strat_test_set): \n",
    "    set_. drop(\"WP_cat\", axis = 1, inplace = True)\n",
    "\n",
    "#Make a copy of train dataset\n",
    "DataSet = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot scatter plot of attributes of interest to identify the correlation between the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix \n",
    "attributes = [\"numerical_attribute_1\",\"numerical_attribute_2\",\"numerical_attribute_3\",\"numerical_attribute_4\"\n",
    "              ,\"numerical_attribute_5\",\"numerical_attribute_6\"]\n",
    "scatter_matrix(DataSet[attributes], figsize =(16, 12))\n",
    "\n",
    "#Check correlations of an attribute of intreset with other attributes in the dataset\n",
    "DataSet.corr()[\"numerical_attribute_1\"].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training data into features and labels. Here the label or target is 'RenewalStatus' with value 'Yes' or 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet = strat_train_set.drop(\"RenewalStatus\", axis = 1) \n",
    "DataSet_labels = strat_train_set[\"RenewalStatus\"]. copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and make sure there are no attributes with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet.columns[DataSet.isnull().any()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define categorical attributes of the training dataset and convert to datatype 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cat_columns = [\"Categorical_attribute_1\", \"Categorical_attribute_2\", \"Categorical_attribute_3\", \"Categorical_attribute_4\"\n",
    "              ,\"Categorical_attribute_5\", \"Categorical_attribute_6\"]\n",
    "\n",
    "for c in Cat_columns:\n",
    "    DataSet[c] = DataSet[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical attributes to numercial factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in category_columns:\n",
    "    DataSet[c],_ = DataSet[c].factorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of data containing only categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSet_cat = DataSet.loc[:,category_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of data containing only numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = list(DataSet.dtypes[DataSet.dtypes == 'object'].index)\n",
    "category_columns = list(DataSet.dtypes[DataSet.dtypes == 'category'].index)\n",
    "\n",
    "DataSet_num = DataSet.drop(object_columns, axis=1)\n",
    "DataSet_num = DataSet.drop(category_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a transformer function to select just a subset of the Pandas DataFrame columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create a class to select numerical or categorical columns \n",
    "# since Scikit-Learn doesn't handle DataFrames yet\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create separate data pipelines to deal with numerical & categorical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical attributes:\n",
    "    #Impute (replace) missing numerical attributes with their median\n",
    "    # Scale the numerical attributes using StandardScaler()\n",
    "\n",
    "#Categorical attributes:\n",
    "    #One hot encoding for categorical attributes\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_attribs = list(DataSet_num)\n",
    "cat_attribs = category_columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', Imputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join the pipelines into a big pipeline that will process both the numerical and the categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.pipeline import FeatureUnion\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "DataSet_prepared = full_pipeline.fit_transform(DataSet)\n",
    "DataSet_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train following models using the training data and find their average accuracy scores using cross validation\n",
    "#### RandomForest\n",
    "#### Support Vector Machine with different kernels\n",
    "#### Logistic regression\n",
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_cl = RandomForestClassifier(random_state=42)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [forest_cl, svm_cl_linear, svm_cl_rbf, svm_cl_poly, svm_cl_sigmoid, svm_cl_linearSVC, logistic, SGD]\n",
    "\n",
    "svm_cl_linear = SVC(kernel=\"linear\",random_state=42)\n",
    "svm_cl_rbf = SVC(kernel=\"rbf\",random_state=42)\n",
    "svm_cl_poly = SVC(kernel=\"poly\",random_state=42)\n",
    "svm_cl_sigmoid = SVC(kernel=\"sigmoid\",random_state=42)\n",
    "svm_cl_linearSVC = LinearSVC(random_state=42)\n",
    "logistic = linear_model.LogisticRegression(random_state=42)\n",
    "SGD = linear_model.SGDClassifier(random_state=42)\n",
    "\n",
    "\n",
    "for m in models:\n",
    "    m.fit(DataSet_prepared, DataSet_labels)\n",
    "    score = cross_val_score(m, DataSet_prepared, DataSet_labels, scoring=\"accuracy\", cv=10,n_jobs=-1)\n",
    "    print(m,': ', '\\n','Mean of Accuracy: ', score.mean(), ', ', 'Standard Deviation of Accuracy: ', score.std(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the model scores; its evident that the promising models are SVM(kernel='rbf') and logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to find the best hyper parameter for Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_iter_search = 7\n",
    "random_search = RandomizedSearchCV(logistic, param_distributions=param_grid,n_iter=n_iter_search,cv=5\n",
    "                                   , random_state=42, n_jobs=-1)\n",
    "random_search.fit(DataSet_prepared,DataSet_labels)\n",
    "\n",
    "print('Best score : ', '\\n',random_search.best_score_)\n",
    "print('Best parameters : ', '\\n',random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to find the best hyper parameters for SVM(kernel='rbf') model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 7, 10)\n",
    "gamma_range = np.logspace(-6, 3, 10)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "random_search = RandomizedSearchCV(svm_cl_rbf, param_distributions=param_grid,cv=5, random_state=42, n_jobs=2)\n",
    "random_search.fit(DataSet_prepared,DataSet_labels)\n",
    "\n",
    "print('Best score : ', '\\n',random_search.best_score_)\n",
    "print('Best parameters : ', '\\n',random_search.best_estimator_)\n",
    "\n",
    "Final_Model = random_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
